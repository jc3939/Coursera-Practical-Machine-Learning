test$classe
train = read.csv("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
train$classe
train.names()
s1=0.6
s2=0.68
n1 = 10
n2 = 10
de = (s1/n1+s2/n2)^2
nu = (s1/n1+s2/n2)^2
de = (s1/n1)^2/(n1−1)+(s2/n2)2/(n2−1)
de = (s1/n1)^2/(n1-1)+(s2/n2)2/(n2−1)
de = (s1/n1)^2/(n1-1)+(s2/n2)^2/(n2−1)
de = (s1/n1)^2/(n1-1)+(s2/n2)^2/(n2-1)
nu/de
(9*s1+9*s2)/8
-2+2.101*sqrt(1.44)
-2-2.101*sqrt(1.44)
-2-2.101*sqrt(1.44)/sqrt(20)
-2+2.101*sqrt(1.44)/sqrt(20)
-2+2.878*sqrt(1.44)/sqrt(20)
-2-2.878*sqrt(1.44)/sqrt(20)
s1=0.5^2
s2=4
n1 = 100
n2 = 200
de = (s1/n1)^2/(n1-1)+(s2/n2)^2/(n2-1)
nu = (s1/n1+s2/n2)^2
nu/de
sqrt(244.1899)/sqrt(200)
-2+1.96*sqrt(244.1899)/sqrt(200)
2-1.96*sqrt(244.1899)/sqrt(200)
s1=1.5^2
s2 = 1.8^2
n2 = 100
s1=0.5^2
s2=4
> de = (s1/n1)^2/(n1-1)+(s2/n2)^2/(n2-1)
> nu = (s1/n1+s2/n2)^2
> de = (s1/n1)^2/(n1-1)+(s2/n2)^2/(n2-1) nu = (s1/n1+s2/n2)^2
de = (s1/n1)^2/(n1-1)+(s2/n2)^2/(n2-1)
nu = (s1/n1+s2/n2)^2
nu/de
2-1.96*sqrt(111.3268)/sqrt(200)
s1=1.5^2
s2 = 1.8^2
n1=9
n2=9
de = (s1/n1)^2/(n1-1)+(s2/n2)^2/(n2-1)
nu = (s1/n1+s2/n2)^2
nu/de
-4+1.746*sqrt(15.4961)/sqrt(18)
-4+1.337*sqrt(15.4961)/sqrt(18)
-4-1.337*sqrt(15.4961)/sqrt(18)
8*s1+8*s2
(8*s1+8*s2)/16
sqrt((8*s1+8*s2)/16)/sqrt(18)
4+1.746*sqrt((8*s1+8*s2)/16)/sqrt(18)
sqrt((8*s1+8*s2)/16)/sqrt(18)
1.5^2*8+1.8^2*8
sd=sqrt(43.92/16)
sd/sqrt(18)*1.746-4
sqrt((8*s1+8*s2)/16)
a = sqrt((8*s1+8*s2)/16)
a*sqrt(1/9+1/9)
-4-1.746+a*sqrt(1/9+1/9)
-4-1.746*a*sqrt(1/9+1/9)
-4+1.746*a*sqrt(1/9+1/9)
attach(mtcars)
head(mtcars)
fit = lm(mpg~cyl+cyl*wt)
summary(fit)
fit1 = lm(mpg~cyl+wt)
summary(fit1)
summary(mtcar$cyl)
cylf = factor(cyl)
fit1 = lm(mpg~cylf+wt)
summary(fit1)
anova(fit1)
summary(mtcars$cyl)
summary(mtcars$cyl1)
anova(fit)
summary(fit1)
anova(fit1)
anova(fit)
fit = lm(mpg~cyl+wt)
summary(fit)
anova(fit)
fit1 = lm(mpg~cylf+wt)
summary(fit1)
anova(fit1)
dim(mtcars)
anova(fit)
summary(fit)
anova(fit1)
412.39/6.54
n <- 100; t <- rep(c(0, 1), c(n/2, n/2)); x <- c(runif(n/2), runif(n/2));
beta0 <- 0; beta1 <- 2; tau <- 1; sigma <- .2
y <- beta0 + x * beta1 + t * tau + rnorm(n, sd = sigma)
plot(x, y, type = "n", frame = FALSE)
abline(lm(y ~ x), lwd = 2)
abline(h = mean(y[1 : (n/2)]), lwd = 3)
abline(h = mean(y[(n/2 + 1) : n]), lwd = 3)
fit <- lm(y ~ x + t)
abline(coef(fit)[1], coef(fit)[2], lwd = 3)
coef(fit)[1]
coef(fit)[2]
abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)
points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = "black", bg = "lightblue", cex = 2)
points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = "black", bg = "salmon", cex = 2)
fit2 <- lm(mpg ~ factor(cyl), mtcars)
summary(fit2)
fit1 = lm(mpg~cylf+wt)
summary(fit1)
fit3 <- lm(mpg ~ factor(cyl)*wt, mtcars)
summary(fit3)
result <- anova(fit1, fit3, test="Chi")
result$Pr
lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
?I
fit4 <- lm(mpg ~ I(wt * 0.5) + factor(cyl), data=mtcars)
summary(fit4)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit5 <- lm(y ~ x)
lm.influence(fit5)$hat[5]
dfbetas(fit5)[5, 2]
x <- -5 : 5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54,
3.87, 4.97)
knotPoint <- c(0)
spline <- sapply(knotPoint, function(knot) (x > knot) * (x - knot))
xMatrix <- cbind(1, x, spline)
fit <- lm(y ~ xMatrix - 1)
yhat <- predict(fit)
yhat
slope <- fit$coef[2] + fit$coef[3]
slope # 1.013
plot(x, y)
lines(x, yhat, col=2)
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot, s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), x.s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), slider = x(0, 2, step = 0.1))
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
library(manipulate)
library(shiny)
data(airquality)
dTable(airquality, sPaginationType = "full_numbers")
library(rCharts)
dTable(airquality, sPaginationType = "full_numbers")
library(shiny)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"),
sidebarPanel(
h2('Big text')
h3('Sidebar')
),
mainPanel(
h3('Main Panel text')
)
))
library(shiny)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"),
sidebarPanel(
h2('Big text'),
h3('Sidebar')
),
mainPanel(
h3('Main Panel text')
)
))
shinyUI(pageWithSidebar(
headerPanel("Example plot"),
sidebarPanel(
sliderInput('mu', 'Guess at the mu',value = 70, min = 60, max = 80, step = 0.05,)
),
mainPanel(
plotOutput('newHist')
)
))
library(UsingR)
data(galton)
shinyServer(
function(input, output) {
output$myHist <- renderPlot({
hist(galton$child, xlab='child height', col='lightblue',main='Histogram')
mu <- input$mu
lines(c(mu, mu), c(0, 200),col="red",lwd=5)
mse <- mean((galton$child - mu)^2)
text(63, 150, paste("mu = ", mu))
text(63, 140, paste("MSE = ", round(mse, 2)))
})
}
)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
?createDataPartition
library(AppliedPredictiveModeling)
library(caret)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
library(ggplot2)
library(caret)
ncol(training)
which(sapply(adData,class)=="factor")
summary(training$diagnosis)
training$diagnosis = as.numeric(training$diagnosis)
p <- prcomp(training[,grep('^IL',names(training))])
p$rotation[,1:7]
qplot(1:length(p$sdev),p$sdev / sum(p$sdev))
which(cumsum(p$sdev) / sum(p$sdev) <= .9)
(cumsum(p$sdev) / sum(p$sdev))[8]
#Result here
preProc <- preProcess(training[,grep('^IL',names(training))],method="pca",thres=.9)
preProc
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
trainSmall <- data.frame(training[,grep('^IL',names(training))],training$diagnosis)
testSmall <- data.frame(testing[,grep('^IL',names(testing))],testing$diagnosis)
preProc <- preProcess(trainSmall[-13],method="pca",thres=.8)
trainPC <- predict(preProc,trainSmall[-13])
testPC <- predict(preProc,testSmall[-13])
PCFit <- train(trainSmall$training.diagnosis~.,data=trainPC,method="glm")
NotPCFit <- train(trainSmall$training.diagnosis~.,data=trainSmall,method="glm")
PCTestPredict <- predict(PCFit,newdata=testPC)
NotPCTestPredict <- predict(NotPCFit,newdata=testSmall)
confusionMatrix(PCTestPredict,testSmall$testing.diagnosis)
confusionMatrix(NotPCTestPredict,testSmall$testing.diagnosis)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
library(ggplot2)
library(caret)
ncol(training)
which(sapply(adData,class)=="factor")
summary(training$diagnosis)
training$diagnosis = as.numeric(training$diagnosis)
p <- prcomp(training[,grep('^IL',names(training))])
p$rotation[,1:7]
qplot(1:length(p$sdev),p$sdev / sum(p$sdev))
which(cumsum(p$sdev) / sum(p$sdev) <= .9)
(cumsum(p$sdev) / sum(p$sdev))[8]
#Result here
preProc <- preProcess(training[,grep('^IL',names(training))],method="pca",thres=.8)
preProc
tf= structure(list(state = structure(1:14, .Label = c("AK", "AL",
"AR", "AZ", "CA", "CO", "CT", "DE", "FL", "GA", "IA", "IL", "IN",
"KS"), class = "factor"), num = c(21L, 31L, 12L, 56L, 316L, 53L,
31L, 7L, 335L, 63L, 42L, 73L, 40L, 2L), region = structure(c(2L,
1L, 4L, 3L, 5L, 6L, 7L, 8L, 9L, 10L, 13L, 11L, 12L, 14L), .Label = c("alabama",
"alaska", "arizona", "arkansas", "california", "colorado", "connecticut",
"delaware", "florida", "georgia", "illinois", "indiana", "iowa",
"kansas"), class = "factor")), .Names = c("state", "num", "region"
), class = "data.frame", row.names = c(NA, -14L))
View(tf)
library(ggplot2)
library(maps)
install.packages("maps")
require(maps)
library(maps)
states <- map_data("state")
tfmerged <- merge(states, tf, sort = FALSE, by = "region")
tfmerged <- tfmerged[order(tfmerged$order), ]
qplot(long, lat, data = tfmerged, group = group, fill = num,
geom="polygon")
View(states)
View(tf)
state <- read.csv("~/Documents/state.csv", sep=";")
View(state)
states <- map_data("state")
tfmerged <- merge(states, state, sort = FALSE, by = "region")
tfmerged <- tfmerged[order(tfmerged$order), ]
qplot(long, lat, data = tfmerged, group = group, fill = num,
geom="polygon")
qplot(long, lat, data = tfmerged, group = group, fill = freq,
geom="polygon")
tfmerged <- merge(states, state, sort = FALSE, by = "region")
View(states)
View(state)
state <- read.csv("~/Documents/state.csv", sep=";")
View(state)
tfmerged <- merge(states, state, sort = FALSE, by = "region")
tfmerged <- tfmerged[order(tfmerged$order), ]
qplot(long, lat, data = tfmerged, group = group, fill = freq,
geom="polygon")
?qplot
?map_date
?map_data
View(tfmerged)
tfmerged <- tfmerged[order(tfmerged$order, decreasing = T), ]
qplot(long, lat, data = tfmerged, group = group, fill = freq,
geom="polygon")
?heatmap2
?heatmap.2
heatmap.2 {gplots}
?heatmap.2 {gplots}
library(gplots)
install.packages("gplots")
library(gplots)
?heatmap.2
?heatmap
x  <- as.matrix(mtcars)
View(x)
rc <- rainbow(nrow(x), start = 0, end = .3)
cc <- rainbow(ncol(x), start = 0, end = .3)
heatmap(x, col = cm.colors(256), scale = "column",
RowSideColors = rc, ColSideColors = cc, margins = c(5,10),
xlab = "specification variables", ylab =  "Car Models",
main = "heatmap(<Mtcars data>, ..., scale = \"column\")")
View(x)
View(tfmerged)
heatmap(long, lat, data = tfmerged, group = group, fill = freq)
qplot(long, lat, data = tfmerged, group = group, fill = freq,
geom="polygon")
qplot(long, lat, data = tfmerged, group = group,
geom="polygon")
qplot(long, lat, data = tfmerged, group = group, fill = freq,
geom="polygon")
?qplot
qplot(long, lat, data = tfmerged, group = group, fill = freq,
geom="polygon")
View(tfmerged)
qplot(long, lat, data = tfmerged, group = group, fill = "red",
geom="polygon")
library(shiny)
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
library(shiny)
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), slider = x(0, 2, step = 0.1))
library(rChart)
library(rCharts)
library(airquality)
data(airquality)
dTable(airquality, sPaginationType = "full_numbers")
shinyUI(pageWithSidebar(
headerPanel("Example plot"),
sidebarPanel(
sliderInput('mu', 'Guess at the mu',value = 70, min = 60, max = 80, step = 0.05,)
),
mainPanel(
plotOutput('newHist')
)
))
library(UsingR)
data(galton)
shinyServer(
function(input, output) {
output$myHist <- renderPlot({
hist(galton$child, xlab='child height', col='lightblue',main='Histogram')
mu <- input$mu
lines(c(mu, mu), c(0, 200),col="red",lwd=5)
mse <- mean((galton$child - mu)^2)
text(63, 150, paste("mu = ", mu))
text(63, 140, paste("MSE = ", round(mse, 2)))
})
}
)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p=3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(ss, method='pca', thresh=0.8,
outcome=training$diagnosis)
preProc$rotation
clear
cls
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p=3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
IL <- grep("^IL", colnames(training), value=TRUE)
ILpredictors <- predictors[, IL]
df <- data.frame(diagnosis, ILpredictors)
inTrain <- createDataPartition(df$diagnosis, p=3/4)[[1]]
training <- df[inTrain, ]
testing <- df[-inTrain, ]
modelFit <- train(diagnosis ~ ., method="glm", data=training)
predictions <- predict(modelFit, newdata=testing)
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
acc1 <- C1$overall[1]
acc1
pmlTraining <- read.csv("~/Downloads/pml-training.csv")
View(pmlTraining)
setwd("~/Documents/JHU_Practical_Machine_Learning")
library(caret)
library(kernlab)
data(spam)
#data slicing
inTrain <- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
hist(training$capitalAve, main = "", xlab = "ave. capital run length")
summary(training)
mean(training$capitalAve)
sd(training$capitalAve)
predict(preObj, training[,-58])$capitalAve
preObj <- preProcess(training[, -58], method = c("center", "scale"))
predict(preObj, training[,-58])$capitalAve
testCapAveS <- predict(preObj, testing[,-58])$capitalAve
mean(testCapAves)
mean(testCapAveS)
set.seed(32343)
modelFit <- train(type ~., data = training, preProcess=c("center", "scale"), method="glm")
modelFit
trainCapAves <- predict(preObj, training[,-58])$capitalAve
preObj <- preProcess(training[, -58], method = c("boxcox"))
preObj <- preProcess(training[, -58], method = c("BoxCox"))
trainCapAves <- predict(preObj, training[,-58])$capitalAve
par(mfrow=c(1,2));
trainCapAveS <- predict(preObj, training[,-58])$capitalAve
hist(trainCapAveS)
qqnorm(trainCapAveS)
#Impute and standardization
set.seed(13343)
#making NA
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1], size = 1, prob=0.05)==1
training$capAve[selectNA] <- NA
preObj <- preProcess(training[,-58], method="knnImpute")
capAve <- predict(preObj, training[,-58])$capAve
capAve <- predict(preObj, training[,-58])$capAve
install.packages("RANN")
capAve <- predict(preObj, training[,-58])$capAve
preObj <- preProcess(training[,-58], method="knnImpute")
capAve <- predict(preObj, training[,-58])$capAve
library(ISLR)
library(ggplot2)
library(caret)
data(Wage)
summary(Wage)
inTrain <- createDataPartition(y = Wage$wage, p = 0.75, list = FALSE)
training <- Wage[inTrain, ]
testing <- Wage[-inTrain, ]
dummies <- dummyVars(wage~jobclass, data=training)
head(predict(dummies, newdata=training))
nsv <- nearZeroVar(training, saveMetrics=TRUE)
nsv
summary(training$sex)
library(splines)
bsBasis <- bs(training$age, df = 3)
bsBasis
head(bsBasis)
M <- abs(cor(training[,-58]))
inTrain <- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
M <- abs(cor(training[,-58]))
M
View(M)
diag(M)<-0
View(M)
which(M>0.8, arr.ind=T)
typeColor<-((spam$type=="spam")*1+1)
prComp<-prcomp(log10(spam[,-58]+1))
plot(prComp$x[,1], prComp$x[,2], col=typeColor, xlab="PC1",ylab="PC2")
preProc <- preProcess(log10(spam[,-58]+1), method="pca", pcaComp=2)
spamPC<-predict(preProc, log10(spam[,-58]+1))
plot(spamPC[,1], spamPC[,2], col=typeColor)
modelFit<-train(training$type~., method="glm", data=trainPC)
trainPC <- predict(preProc, log10(spam[,-58]+1))
modelFit<-train(training$type~., method="glm", data=trainPC)
preProc <- preProcess(log10(train[,-58]+1), method="pca", pcaComp=2)
preProc <- preProcess(log10(training[,-58]+1), method="pca", pcaComp=2)
trainPC <- predict(preProc, log10(spam[,-58]+1))
modelFit<-train(training$type~., method="glm", data=trainPC)
trainPC <- predict(preProc, log10(training[,-58]+1))
modelFit<-train(training$type~., method="glm", data=trainPC)
modelFit<-train(training$type~., method="glm", data=training, preProcess="pca")
confusionMatrix(tesing$type, predict(modelFit, tesing))
confusionMatrix(tesing$type, predict(modelFit, testing))
confusionMatrix(testing$type, predict(modelFit, testing))
